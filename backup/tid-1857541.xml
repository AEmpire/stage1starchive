<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>求问 怎么抓取页面上的多个表格数据啊？Try5</title>
    <link>https://bbs.saraba1st.com/2b/</link>
    <description>求问 怎么抓取页面上的多个表格数据啊？Try5</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <lastBuildDate>Thu, 09 Jul 2020 17:53:37 +0000</lastBuildDate>
    <item>
      <title>求问 怎么抓取页面上的多个表格数据啊？Try5[0-50]</title>
      <link>https://bbs.saraba1st.com/2b/thread-1857541-1-1.html</link>
      <description>求问 怎么抓取页面上的多个表格数据啊？Try5&#13;
 本帖最后由 佐塚間桐 于 2019-10-5 23:41 编辑 

目标网页如下
&#13;
http://ro321.com/index.php?page=item_db&amp;item_type=11
&#13;
其实是多个页面，每页都有多个表格。我本来是想把所有页面的表格按原格式提取出来生成单一html文件，但是完全不知道怎么弄
&#13;
退而求其次，想到可以用python抓表格的数据放到一个excel表格里面
&#13;
结果图片不知道怎么抓，只好暂时先放一放，先管数据
&#13;
多个页面，用个循环应该就可以了，但是怎么设置抓取时间间隔，万一对方以为我恶意拖库怎么办，还是先抓一页试试看
&#13;
表格格式有点复杂，还是换成简单格式。先参考别人的代码改一改吧
&#13;
# -*- coding: utf-8 -*-
&#13;
import requests
&#13;
from bs4 import BeautifulSoup
&#13;
import pandas as pd
&#13;
from lxml import etree
&#13;
def itemQuery():
&#13;
    url = 'http://ro321.com/index.php?page=item_db&amp;item_type=11'
&#13;
    res = requests.get(url)
&#13;
    print(res.content)
&#13;
    soup = BeautifulSoup(res.content, "html.parser")
&#13;
    table = soup.find_all('table', attrs={"class": "content_box_item"}).decode()
&#13;
    print(table)
&#13;
    print(type(table))
&#13;
    df = pd.read_html(table, encoding='utf-8', header=0)
&#13;
    results = list(df.T.to_dict().values())
&#13;
    print(results)
&#13;
    df.to_csv("./item.csv", index=False)
&#13;
if __name__ =="__main__":
&#13;
      itemQuery()
&#13;
好！F5走起
&#13;
等了一分钟，一点反应都没有？这是怎么回事？求解
&#13;
==============================================
&#13;
换了另外一种方法
&#13;
经过提醒，既然 itembox content中间被打断了，就再在检查里面看看哪一栏包括了单个table的全部内容
&#13;
发现 tbody好像也是包括单独一整个table的N内容，就用这个定位吧
&#13;
# -*- coding: utf-8 -*-
&#13;
import requests
&#13;
from bs4 import BeautifulSoup
&#13;
import pandas as pd
&#13;
from lxml import etree
&#13;
def run():
&#13;
    url="http://ro321.com/index.php?page=item_db&amp;item_type=11"
&#13;
    req=requests.get(url)
&#13;
    mylist=[["物品"],["类别"],["系列"],["买价"],["卖价"],["重量"],["功能说明"],["作用剧本"],["掉落怪物"],["商店销售点"],["其他获得方式"]]
&#13;
    soup=BeautifulSoup(req.content,"html.parser")
&#13;
    for it in soup.find_all('tbody',attrs={"tbody"}):
&#13;
      # print(it)
&#13;
      for i in range(len(it.contents)):
&#13;
            # print(i)
&#13;
            data = it.contents.contents.text
&#13;
            # print(data)
&#13;
            mylist.append(data)
&#13;
            print(mylist)
&#13;
      print(mylist)
&#13;
    print(mylist)
&#13;
    out=pd.DataFrame(mylist)
&#13;
    out.to_excel(r'1.xls')
&#13;
if __name__ =="__main__":
&#13;
    run()
&#13;
F5走起！
&#13;
这回顺利运行了，也生成了excel文件，但是好像内容一个都没抓到啊？就print了我自己输入的表头
&#13;
是说明tbody定位没定到吗？
&#13;
==================================================
&#13;
这回用笨办法，一个值一个值的取
&#13;
一步一步试
&#13;
[*]# -*- coding: utf-8 -*-
&#13;
[*]
&#13;
[*]import requests
&#13;
[*]from bs4 import BeautifulSoup
&#13;
[*]import pandas as pd
&#13;
[*]from lxml import etree
&#13;
[*]
&#13;
[*]def getTableText():
&#13;
[*]    url = "http://ro321.com/index.php?page=item_db&amp;item_type=11"
&#13;
[*]    res = requests.get(url)
&#13;
[*]    res_elements = etree.HTML(res.text)
&#13;
[*]    name=res_elements.xpath('//table[@width="720px"]/tbody/tr[@class="lmd"]/td/table/tbody/tr/td/b').text
&#13;
[*]    print(name)
&#13;
[*]
&#13;
[*]if __name__ =="__main__":
&#13;
[*]    getTableText()
&#13;
复制代码
&#13;
xpath（table[@width="720px"]）可以准确取到中间12个物品的table
&#13;
后面跟的那一段是多层嵌套剥皮之后看到的加黑的物品名称
&#13;
但是F5走起，结果显示IndexError: list index out of range
&#13;
这又是错在哪里了？
&#13;
==============================================================
&#13;
很顺利把第一个table需要的数据都抓取到了
&#13;
import requests
&#13;
from bs4 import BeautifulSoup
&#13;
import pandas as pd
&#13;
from lxml import etree
&#13;
import chardet
&#13;
def getTableText():
&#13;
    url = "http://ro321.com/index.php?page=item_db&amp;item_type=11"
&#13;
    res = requests.get(url)
&#13;
    res.encoding='gbk'
&#13;
    res_elements = etree.HTML(res.text,parser=etree.HTMLParser(encoding='gbk'))
&#13;
    id=res_elements.xpath('string(//table[@width="720px"]/tr[@class="lmd"]/td/table/tr/td)')
&#13;
    print(id)
&#13;
    purchaseprice=res_elements.xpath('string(//table[@width="720px"]/tr/td)')
&#13;
    print(purchaseprice)
&#13;
    weight=res_elements.xpath('string(//table[@width="720px"]/tr/td)')
&#13;
    print(weight)
&#13;
    functionInstruction=res_elements.xpath('string(//table[@width="720px"]/tr/td)')
&#13;
    print(functionInstruction)
&#13;
    drops=res_elements.xpath('string(//table[@width="720px"]/tr/td/table)')
&#13;
    print(drops)
&#13;
    altAcquire=res_elements.xpath('string(//table[@width="720px"]/tr/td)')
&#13;
    print(altAcquire)
&#13;
if __name__ =="__main__":
&#13;
    getTableText()
&#13;
接下来以为只要再在外面套个循环，就可以依次获取其他表格的数据，放到list里面就解决问题了
&#13;
结果发现远没有想象的简单。
&#13;
xpath返回的是list，本来想的是xpath语句中table[@width="720px"] 后面加定位每一张表格，i从1走到12就遍历了
&#13;
但xpath括号内不允许传递参数，外面套循环是没用的，每次返回的都是第一个table的结果，这就傻逼了
&#13;
请问这种情况有什么办法可以实现依次读取各个table的内容吗？
&#13;
解决了，传参数进去可以用format</description>
      <content:encoded><![CDATA[<p><b>佐塚間桐: </b><br>
<span>求问 怎么抓取页面上的多个表格数据啊？Try5</span><br>
<span> 本帖最后由 佐塚間桐 于 2019-10-5 23:41 编辑 </span><br>
<span>目标网页如下</span><br>
<span>http://ro321.com/index.php?page=item_db&item_type=11</span><br>
<span>其实是多个页面，每页都有多个表格。我本来是想把所有页面的表格按原格式提取出来生成单一html文件，但是完全不知道怎么弄</span><br>
<span>退而求其次，想到可以用python抓表格的数据放到一个excel表格里面</span><br>
<span>结果图片不知道怎么抓，只好暂时先放一放，先管数据</span><br>
<span>多个页面，用个循环应该就可以了，但是怎么设置抓取时间间隔，万一对方以为我恶意拖库怎么办，还是先抓一页试试看</span><br>
<span>表格格式有点复杂，还是换成简单格式。先参考别人的代码改一改吧</span><br>
<span># -*- coding: utf-8 -*-</span><br>
<span>import requests</span><br>
<span>from bs4 import BeautifulSoup</span><br>
<span>import pandas as pd</span><br>
<span>from lxml import etree</span><br>
<span>def itemQuery():</span><br>
<span>    url = 'http://ro321.com/index.php?page=item_db&item_type=11'</span><br>
<span>    res = requests.get(url)</span><br>
<span>    print(res.content)</span><br>
<span>    soup = BeautifulSoup(res.content, "html.parser")</span><br>
<span>    table = soup.find_all('table', attrs={"class": "content_box_item"}).decode()</span><br>
<span>    print(table)</span><br>
<span>    print(type(table))</span><br>
<span>    df = pd.read_html(table, encoding='utf-8', header=0)</span><br>
<span>    results = list(df.T.to_dict().values())</span><br>
<span>    print(results)</span><br>
<span>    df.to_csv("./item.csv", index=False)</span><br>
<span>if __name__ =="__main__":</span><br>
<span>      itemQuery()</span><br>
<span>好！F5走起</span><br>
<span>等了一分钟，一点反应都没有？这是怎么回事？求解</span><br>
<span>==============================================</span><br>
<span>换了另外一种方法</span><br>
<span>经过提醒，既然 itembox content中间被打断了，就再在检查里面看看哪一栏包括了单个table的全部内容</span><br>
<span>发现 tbody好像也是包括单独一整个table的N内容，就用这个定位吧</span><br>
<span># -*- coding: utf-8 -*-</span><br>
<span>import requests</span><br>
<span>from bs4 import BeautifulSoup</span><br>
<span>import pandas as pd</span><br>
<span>from lxml import etree</span><br>
<span>def run():</span><br>
<span>    url="http://ro321.com/index.php?page=item_db&item_type=11"</span><br>
<span>    req=requests.get(url)</span><br>
<span>    mylist=[["物品"],["类别"],["系列"],["买价"],["卖价"],["重量"],["功能说明"],["作用剧本"],["掉落怪物"],["商店销售点"],["其他获得方式"]]</span><br>
<span>    soup=BeautifulSoup(req.content,"html.parser")</span><br>
<span>    for it in soup.find_all('tbody',attrs={"tbody"}):</span><br>
<span>      # print(it)</span><br>
<span>      for i in range(len(it.contents)):</span><br>
<span>            # print(i)</span><br>
<span>            data = it.contents.contents.text</span><br>
<span>            # print(data)</span><br>
<span>            mylist.append(data)</span><br>
<span>            print(mylist)</span><br>
<span>      print(mylist)</span><br>
<span>    print(mylist)</span><br>
<span>    out=pd.DataFrame(mylist)</span><br>
<span>    out.to_excel(r'1.xls')</span><br>
<span>if __name__ =="__main__":</span><br>
<span>    run()</span><br>
<span>F5走起！</span><br>
<span>这回顺利运行了，也生成了excel文件，但是好像内容一个都没抓到啊？就print了我自己输入的表头</span><br>
<span>是说明tbody定位没定到吗？</span><br>
<span>==================================================</span><br>
<span>这回用笨办法，一个值一个值的取</span><br>
<span>一步一步试</span><br>
<span>[*]# -*- coding: utf-8 -*-</span><br>
<span>[*]</span><br>
<span>[*]import requests</span><br>
<span>[*]from bs4 import BeautifulSoup</span><br>
<span>[*]import pandas as pd</span><br>
<span>[*]from lxml import etree</span><br>
<span>[*]</span><br>
<span>[*]def getTableText():</span><br>
<span>[*]    url = "http://ro321.com/index.php?page=item_db&item_type=11"</span><br>
<span>[*]    res = requests.get(url)</span><br>
<span>[*]    res_elements = etree.HTML(res.text)</span><br>
<span>[*]    name=res_elements.xpath('//table[@width="720px"]/tbody/tr[@class="lmd"]/td/table/tbody/tr/td/b').text</span><br>
<span>[*]    print(name)</span><br>
<span>[*]</span><br>
<span>[*]if __name__ =="__main__":</span><br>
<span>[*]    getTableText()</span><br>
<span>复制代码</span><br>
<span>xpath（table[@width="720px"]）可以准确取到中间12个物品的table</span><br>
<span>后面跟的那一段是多层嵌套剥皮之后看到的加黑的物品名称</span><br>
<span>但是F5走起，结果显示IndexError: list index out of range</span><br>
<span>这又是错在哪里了？</span><br>
<span>==============================================================</span><br>
<span>很顺利把第一个table需要的数据都抓取到了</span><br>
<span>import requests</span><br>
<span>from bs4 import BeautifulSoup</span><br>
<span>import pandas as pd</span><br>
<span>from lxml import etree</span><br>
<span>import chardet</span><br>
<span>def getTableText():</span><br>
<span>    url = "http://ro321.com/index.php?page=item_db&item_type=11"</span><br>
<span>    res = requests.get(url)</span><br>
<span>    res.encoding='gbk'</span><br>
<span>    res_elements = etree.HTML(res.text,parser=etree.HTMLParser(encoding='gbk'))</span><br>
<span>    id=res_elements.xpath('string(//table[@width="720px"]/tr[@class="lmd"]/td/table/tr/td)')</span><br>
<span>    print(id)</span><br>
<span>    purchaseprice=res_elements.xpath('string(//table[@width="720px"]/tr/td)')</span><br>
<span>    print(purchaseprice)</span><br>
<span>    weight=res_elements.xpath('string(//table[@width="720px"]/tr/td)')</span><br>
<span>    print(weight)</span><br>
<span>    functionInstruction=res_elements.xpath('string(//table[@width="720px"]/tr/td)')</span><br>
<span>    print(functionInstruction)</span><br>
<span>    drops=res_elements.xpath('string(//table[@width="720px"]/tr/td/table)')</span><br>
<span>    print(drops)</span><br>
<span>    altAcquire=res_elements.xpath('string(//table[@width="720px"]/tr/td)')</span><br>
<span>    print(altAcquire)</span><br>
<span>if __name__ =="__main__":</span><br>
<span>    getTableText()</span><br>
<span>接下来以为只要再在外面套个循环，就可以依次获取其他表格的数据，放到list里面就解决问题了</span><br>
<span>结果发现远没有想象的简单。</span><br>
<span>xpath返回的是list，本来想的是xpath语句中table[@width="720px"] 后面加定位每一张表格，i从1走到12就遍历了</span><br>
<span>但xpath括号内不允许传递参数，外面套循环是没用的，每次返回的都是第一个table的结果，这就傻逼了</span><br>
<span>请问这种情况有什么办法可以实现依次读取各个table的内容吗？</span><br>
<span>解决了，传参数进去可以用format</span><br>
</p><p><b>BRRM: </b><br>
<span> 本帖最后由 BRRM 于 2019-10-2 23:57 编辑 </span><br>
<span><table class = 'content_box_item' border = 1 cellspacing=0 cellpadding=3 width = '720px'></span><br>
<span>      <tr class = 'lmd'></span><br>
<span>                <td class = 'bborder' align = 'left' colspan=10></span><br>
<span>                        <table class='btext' width='100%'><tr></span><br>
<span>                              <td width=24><img src = 'http://file5.ratemyserver.net/items/small/601.gif' border = '0' onMouseOver = "ddrivetip_image('<img src = \'http://file5.ratemyserver.net/items/large/601.gif\' border = \'1\'>')" onMouseOut = "hideddrivetip_image()"></td></span><br>
<span>                              <td valign='bottom'><b> 苍蝇翅膀 </b>   物品 ID# 601 (Wing_Of_Fly)</td></td></span><br>
<span>                              <td valign='bottom' align='right'></span><br>
<span>                              <a href='index.php?page=re_item_db&item_id=601&ird=0' title='查看这物品复兴后的资料' onclick='return popItem_re("601&ird=0",1,1)'><img src='images/see_renewal.gif' border=0></a></span><br>
<span>                              <a href='item_sprname_search.php?item_id=601'><img src='images/spr.gif' border=0 title='look up sprite name' onclick="return popWin('item_sprname_search.php?item_id=601&small=1', 'name', spr_dim)"></a></span><br>
<span>      </tr></span><br>
<span></table></span><br>
<span>看出问题了吗，第4行的 <table> 在第6行被 </td></td> 提前结束了。所以第11行的</table>把第1行的 <table class = 'content_box_item' ...>给结束了</span><br>
<span>也就是说</span><br>
<span>soup.find_all('table', attrs={"class": "content_box_item"})其实只解析了上面那部分内容。</span><br>
<span>简单的说，这个页面写得很不规范，只有聪明的浏览器才能解决这种不规范问题，这种库解决不了。</span><br>
</p><p><b>精钢魔像: </b><br>
<span>bs4 的解析器你换成lxml试试</span><br>
</p><p><b>startdl: </b><br>
<span>用js呗</span><br>
<span>这句就能获得你要的全部table，然后直接在控制台里复制出来粘贴到本地的html里</span><br>
<span>麻烦的是每个页面要重新复制粘贴一遍</span><br>
<span>let tables = document.getElementsByClassName('content_box_item')</span><br>
</p><p><b>佐塚間桐: </b><br>
<span>BRRM 发表于 2019-10-2 23:43</span><br>
<span>看出问题了吗，第4行的在第6行被提前结束了。所以第11行的把第1行的 给结束了</span><br>
<span>也就是说</span><br>
<span>谢谢</span><br>
<span>html基本不懂没指点确实不知道是这样</span><br>
</p><p><b>佐塚間桐: </b><br>
<span>精钢魔像 发表于 2019-10-2 23:58</span><br>
<span>bs4 的解析器你换成lxml试试</span><br>
<span>嗯，我再试试别的办法看</span><br>
</p><p><b>佐塚間桐: </b><br>
<span>startdl 发表于 2019-10-3 00:23</span><br>
<span>用js呗</span><br>
<span>这句就能获得你要的全部table，然后直接在控制台里复制出来粘贴到本地的html里</span><br>
<span>麻烦的是每个页面要 ...</span><br>
<span>试方法多半比手动慢。但是我贴的这一类是7页，其他的有几十页的，还有一百多页的，不想机械运动</span><br>
</p><p><b>typeunknown: </b><br>
<span>自己写正则吧</span><br>
</p><p><b>startdl: </b><br>
<span>佐塚間桐 发表于 2019-10-3 11:03</span><br>
<span>试方法多半比手动慢。但是我贴的这一类是7页，其他的有几十页的，还有一百多页的，不想机械运动 ...</span><br>
<span>用js的话你的需求实现起来很简单，不过想一键操作会有跨域问题。在浏览器控制台写不会跨域但是麻烦的一笔。</span><br>
<span>不会写爬虫，帮不到你了</span><br>
</p><p><b>佐塚間桐: </b><br>
<span>换了另外一种方法</span><br>
<span>经过提醒，既然 itembox content中间被打断了，就再在检查里面看看哪一栏包括了单个table的全部内容</span><br>
<span>发现 tbody好像也是包括单独一整个table的N内容，就用这个定位吧</span><br>
<span># -*- coding: utf-8 -*-</span><br>
<span>import requests</span><br>
<span>from bs4 import BeautifulSoup</span><br>
<span>import pandas as pd</span><br>
<span>from lxml import etree</span><br>
<span>def run():</span><br>
<span>    url="http://ro321.com/index.php?page=item_db&item_type=11"</span><br>
<span>    req=requests.get(url)</span><br>
<span>    mylist=[["物品"],["类别"],["系列"],["买价"],["卖价"],["重量"],["功能说明"],["作用剧本"],["掉落怪物"],["商店销售点"],["其他获得方式"]]</span><br>
<span>    soup=BeautifulSoup(req.content,"html.parser")</span><br>
<span>    for it in soup.find_all('tbody',attrs={"tbody"}):</span><br>
<span>      # print(it)</span><br>
<span>      for i in range(len(it.contents)):</span><br>
<span>            # print(i)</span><br>
<span>            data = it.contents.contents.text</span><br>
<span>            # print(data)</span><br>
<span>            mylist.append(data)</span><br>
<span>            print(mylist)</span><br>
<span>      print(mylist)</span><br>
<span>    print(mylist)</span><br>
<span>    out=pd.DataFrame(mylist)</span><br>
<span>    out.to_excel(r'1.xls')</span><br>
<span>if __name__ =="__main__":</span><br>
<span>    run()</span><br>
<span>F5走起！</span><br>
<span>这回顺利运行了，也生成了excel文件，但是好像内容一个都没抓到啊？就print了我自己输入的表头</span><br>
<span>是说明tbody定位没定到吗？</span><br>
</p><p><b>cxf5102: </b><br>
<span>这网页table套table的把👴看麻了，我寻思你要是想保留原格式的话够你喝一壶</span><br>
<span>我寻思要么你（JS） document.querySelectorAll('table.content_box_item') ，取innerText，再正则处理</span><br>
<span>要么按照表结构写选择器，感觉工作量应该差不多</span><br>
</p><p><b>kinta: </b><br>
<span>试试pyquery </span><br>
<span>—— 来自 samsung SM-G9600, Android 9上的 S1Next-鹅版 v2.1.2</span><br>
</p><p><b>佐塚間桐: </b><br>
<span>cxf5102 发表于 2019-10-3 18:06</span><br>
<span>这网页table套table的把👴看麻了，我寻思你要是想保留原格式的话够你喝一壶</span><br>
<span>我寻思要么你（JS） document.q ...</span><br>
<span>请问选择器的思路是什么呢？</span><br>
<span>我现在想到的是干脆不管什么table不table了，直接用每个关键字定位，读取后面那一段文本，这样图片也可以一起下载</span><br>
</p><p><b>cxf5102: </b><br>
<span>佐塚間桐 发表于 2019-10-3 21:32</span><br>
<span>请问选择器的思路是什么呢？</span><br>
<span>我现在想到的是干脆不管什么table不table了，直接用每个关键字定位，读取后 ...</span><br>
<span>我看这样</span><br>
<span>document.querySelectorAll('table.content_box_item > tbody > tr') 这样选出来的nodelist，每个元素就是表的一行，你要分列就再写细点</span><br>
<span>document.querySelectorAll('table.content_box_item tr.lmd td > img') 选择所有图标，你要的应该是大图，在元素属性里有</span><br>
<span>你用这两个选择器做应该就可以了，前一个取innerText，后一个取得图片地址再下图片</span><br>
</p><p><b>win8: </b><br>
<span>搭车问个问题我有几十个log文件，都是text/csv格式，有没有同时打开同步滚动高亮差异内容的工具？</span><br>
</p><p><b>佐塚間桐: </b><br>
<span>cxf5102 发表于 2019-10-3 22:02</span><br>
<span>我看这样</span><br>
<span>document.querySelectorAll('table.content_box_item > tbody > tr') 这样选出来的nodelist，每 ...</span><br>
<span>啊，你说的是js吗？不会啊</span><br>
</p><p><b>佐塚間桐: </b><br>
<span> 本帖最后由 佐塚間桐 于 2019-10-3 22:56 编辑 </span><br>
<span>有个思路是先把所有文本都提取出来，然后做文本分析</span><br>
<span>这样有几个问题，一是旁边的广告和链接表格也会一起提取，到时候文本就会很乱。所以必须只提取中间有效内容的那部分文本，如何定位呢</span><br>
<span>解决问题一之后，提取了文本，接下来如何区分不同表格。因为光从文本上看上一个表格最后的文本和下一个表格的第一行没有统一的关键字来区分</span><br>
<span>还有就是到时候如何把图片对应到相应的物品后面。图片可以根据物品id对应上去，这个好像不是特别大的问题</span><br>
</p><p><b>startdl: </b><br>
<span>win8 发表于 2019-10-3 22:05</span><br>
<span>搭车问个问题我有几十个log文件，都是text/csv格式，有没有同时打开同步滚动高亮差异内容的工具？ ...</span><br>
<span>百度一下Diffchecker 有各种查差异内容的网站</span><br>
<span>不过只能对比2个文件 多个文件的没看过，毕竟很少有这种需求</span><br>
</p><p><b>佐塚間桐: </b><br>
<span>文本方式没充分利用html解析器的功能</span><br>
<span>新的想法是观察html格式，精确定位。</span><br>
</p><p><b>佐塚間桐: </b><br>
<span>这回用笨办法，一个值一个取一步一步试</span><br>
<span># -*- coding: utf-8 -*-</span><br>
<span>import requests</span><br>
<span>from bs4 import BeautifulSoup</span><br>
<span>import pandas as pd</span><br>
<span>from lxml import etree</span><br>
<span>def getTableText():</span><br>
<span>    url = "http://ro321.com/index.php?page=item_db&item_type=11"</span><br>
<span>    res = requests.get(url)</span><br>
<span>    res_elements = etree.HTML(res.text)</span><br>
<span>    name=res_elements.xpath('//table[@width="720px"]/tbody/tr[@class="lmd"]/td/table/tbody/tr/td/b').text</span><br>
<span>    print(name)</span><br>
<span>if __name__ =="__main__":</span><br>
<span>    getTableText()xpath（table[@width="720px"]）可以准确取到中间12个物品的table</span><br>
<span>后面跟的那一段是多层嵌套剥皮之后看到的加黑的物品名称</span><br>
<span>但是F5走起，结果显示IndexError: list index out of range</span><br>
<span>这又是错在哪里了？</span><br>
</p><p><b>rrpw777: </b><br>
<span>用js操纵无头chrome的库Puppeteer。我的笨办法：根据content_box_item这个选择器找到表格，存数组里。如果页面上有下一页这个按钮就点一下，再找表格存好。没有下一页就退出程序。合并表格数组之后加上头尾生成完整html写入文件。应该有更好的思路。查了一下python下有一个第三方的port叫pyppeteer，楼主有兴趣可以试试。</span><br>
<span>就只把7页content_box_item取出来了。</span><br>
<span>链接: https://pan.baidu.com/s/1yBjOelMcjRPKrX98W35iSQ 提取码: st8e</span><br>
</p><p><b>佐塚間桐: </b><br>
<span> 本帖最后由 佐塚間桐 于 2019-10-5 01:22 编辑 </span><br>
<span>rrpw777 发表于 2019-10-4 17:16</span><br>
<span>用js操纵无头chrome的库Puppeteer。我的笨办法：根据content_box_item这个选择器找到表格，存数组里。如果 ...</span><br>
<span>谢谢，这个格式就是最开始想的那种。</span><br>
<span>我看看能不能在py实现</span><br>
<span>这个看了感觉实现困难</span><br>
<span>之前自己的那种读取出的问题搞半天是html自己生成的tbody，去掉tbody就好了</span><br>
<span>现在已经能正常读取名字，但还有问题。在python里面输出显示的是乱码： ²ÔÓ¬³á°ò</span><br>
<span>查了一下，原始编码应该是gbk，输出的编码是window1252</span><br>
<span>但搞了半天也没正常输出，郁闷</span><br>
</p><p><b>佐塚間桐: </b><br>
<span>现在已经能正常读取名字，但还有问题。在python里面输出显示的是乱码： ²ÔÓ¬³á°ò</span><br>
<span>查了一下，原始编码应该是gbk，输出的编码是window1252</span><br>
<span>但搞了半天也没正常输出，郁闷</span><br>
</p><p><b>精钢魔像: </b><br>
<span>佐塚間桐 发表于 2019-10-5 01:22</span><br>
<span>现在已经能正常读取名字，但还有问题。在python里面输出显示的是乱码： ²ÔÓ¬³á°ò</span><br>
<span>查了一下，原始 ...</span><br>
<span>http://cn.python-requests.org/zh_CN/latest/user/quickstart.html#id3</span><br>
<span>指定下编码</span><br>
<span>或者取字节流自己转字符串</span><br>
</p><p><b>佐塚間桐: </b><br>
<span>精钢魔像 发表于 2019-10-5 02:39</span><br>
<span>http://cn.python-requests.org/zh_CN/latest/user/quickstart.html#id3</span><br>
<span>指定下编码</span><br>
<span>谢谢，取链接之后加一行res.encoding=gbk就可以了</span><br>
<span>在etree的地方指定没用，必须在调用res.text之前加才行</span><br>
</p><p><b>佐塚間桐: </b><br>
<span> 本帖最后由 佐塚間桐 于 2019-10-5 23:41 编辑 </span><br>
<span>很顺利把第一个table需要的数据都抓取到了</span><br>
<span>import requests</span><br>
<span>from bs4 import BeautifulSoup</span><br>
<span>import pandas as pd</span><br>
<span>from lxml import etree</span><br>
<span>import chardet</span><br>
<span>def getTableText():</span><br>
<span>    url = "http://ro321.com/index.php?page=item_db&item_type=11"</span><br>
<span>    res = requests.get(url)</span><br>
<span>    res.encoding='gbk'</span><br>
<span>    res_elements = etree.HTML(res.text,parser=etree.HTMLParser(encoding='gbk'))</span><br>
<span>    id=res_elements.xpath('string(//table[@width="720px"]/tr[@class="lmd"]/td/table/tr/td)')</span><br>
<span>    print(id)</span><br>
<span>    purchaseprice=res_elements.xpath('string(//table[@width="720px"]/tr/td)')</span><br>
<span>    print(purchaseprice)</span><br>
<span>    weight=res_elements.xpath('string(//table[@width="720px"]/tr/td)')</span><br>
<span>    print(weight)</span><br>
<span>    functionInstruction=res_elements.xpath('string(//table[@width="720px"]/tr/td)')</span><br>
<span>    print(functionInstruction)</span><br>
<span>    drops=res_elements.xpath('string(//table[@width="720px"]/tr/td/table)')</span><br>
<span>    print(drops)</span><br>
<span>    altAcquire=res_elements.xpath('string(//table[@width="720px"]/tr/td)')</span><br>
<span>    print(altAcquire)</span><br>
<span>if __name__ =="__main__":</span><br>
<span>    getTableText()</span><br>
<span>接下来以为只要再在外面套个循环，就可以依次获取其他表格的数据，放到list里面就解决问题了</span><br>
<span>结果发现远没有想象的简单。</span><br>
<span>xpath返回的是list，本来想的是xpath语句中table[@width="720px"] 后面加定位每一张表格，i从1走到12就遍历了</span><br>
<span>但xpath括号内不允许传递参数，外面套循环是没用的，每次返回的都是第一个table的结果，这就傻逼了</span><br>
<span>请问这种情况有什么办法可以实现依次读取各个table的内容吗？</span><br>
<span>解决了，传参数进去可以用format</span><br>
</p><p><b>忘归然: </b><br>
<span>我明天用公司的产品试试</span><br>
<span>—— 来自 Google Pixel 2 XL, Android 10上的 S1Next-鹅版 v2.1.2</span><br>
</p>]]></content:encoded>
      <guid isPermaLink="false">1857541[0-50]</guid>
    </item>
  </channel>
</rss>
